Log file: results/cifar10/2021-12-01-19-19-49__resnet34_6839/log.txt
save path : results/cifar10/2021-12-01-19-19-49__resnet34_6839
torch version :1.8.2+cu111
python version :3.7.11 (default, Jul 27 2021, 14:32:16) 
[GCC 7.5.0]
Use Cuda :True
Dataset :cifar10
Network arch :resnet34
Batch size :128
Epochs :200
Random seed :6839
Num Workers :4
Lr Decay Steps size :60
Weight Decay :0.0005
Model :ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
Optimizer : Adam
Epoch 0/199
----------
LR 0.001
train Loss: 1.4199 Acc: 0.4867
val Loss: 1.3187 Acc: 0.5470
saving best model
best_acc 0.547
Model saved to results/cifar10/2021-12-01-19-19-49__resnet34_6839
0m 11s
Epoch 1/199
----------
LR 0.001
train Loss: 1.0170 Acc: 0.6393
val Loss: 1.0360 Acc: 0.6368
saving best model
best_acc 0.6368
Model saved to results/cifar10/2021-12-01-19-19-49__resnet34_6839
0m 12s
Epoch 2/199
----------
LR 0.001
train Loss: 0.8556 Acc: 0.7025
val Loss: 0.9359 Acc: 0.6812
saving best model
best_acc 0.6812
Model saved to results/cifar10/2021-12-01-19-19-49__resnet34_6839
0m 12s
Epoch 3/199
----------
LR 0.001
train Loss: 0.7471 Acc: 0.7391
val Loss: 0.9842 Acc: 0.6671
0m 11s
Epoch 4/199
----------
LR 0.001
train Loss: 0.6638 Acc: 0.7699
val Loss: 0.8052 Acc: 0.7267
saving best model
best_acc 0.7267
Model saved to results/cifar10/2021-12-01-19-19-49__resnet34_6839
0m 12s
Epoch 5/199
----------
LR 0.001
train Loss: 0.5967 Acc: 0.7956
val Loss: 0.7995 Acc: 0.7314
saving best model
best_acc 0.7314
Model saved to results/cifar10/2021-12-01-19-19-49__resnet34_6839
0m 12s
Epoch 6/199
----------
LR 0.001
train Loss: 0.5429 Acc: 0.8141
val Loss: 0.7185 Acc: 0.7564
saving best model
best_acc 0.7564
Model saved to results/cifar10/2021-12-01-19-19-49__resnet34_6839
0m 12s
Epoch 7/199
----------
LR 0.001
train Loss: 0.4883 Acc: 0.8339
val Loss: 0.8851 Acc: 0.7138
0m 11s
Epoch 8/199
----------
LR 0.001
train Loss: 0.4443 Acc: 0.8486
val Loss: 0.7575 Acc: 0.7527
0m 11s
Epoch 9/199
----------
LR 0.001
train Loss: 0.3929 Acc: 0.8669
val Loss: 0.9010 Acc: 0.7188
0m 11s
Epoch 10/199
----------
LR 0.001
train Loss: 0.3613 Acc: 0.8768
val Loss: 0.7741 Acc: 0.7587
saving best model
best_acc 0.7587
Model saved to results/cifar10/2021-12-01-19-19-49__resnet34_6839
0m 12s
Epoch 11/199
----------
LR 0.001
train Loss: 0.3196 Acc: 0.8911
val Loss: 0.7795 Acc: 0.7613
saving best model
best_acc 0.7613
Model saved to results/cifar10/2021-12-01-19-19-49__resnet34_6839
0m 12s
Epoch 12/199
----------
LR 0.001
train Loss: 0.2865 Acc: 0.9028
val Loss: 0.8970 Acc: 0.7428
0m 11s
Epoch 13/199
----------
LR 0.001
train Loss: 0.2680 Acc: 0.9102
val Loss: 0.8062 Acc: 0.7571
0m 11s
Epoch 14/199
----------
LR 0.001
train Loss: 0.2419 Acc: 0.9184
val Loss: 0.8964 Acc: 0.7424
0m 11s
Epoch 15/199
----------
LR 0.001
train Loss: 0.2220 Acc: 0.9256
val Loss: 0.8229 Acc: 0.7625
saving best model
best_acc 0.7625
Model saved to results/cifar10/2021-12-01-19-19-49__resnet34_6839
0m 12s
Epoch 16/199
----------
LR 0.001
train Loss: 0.2091 Acc: 0.9287
val Loss: 0.9016 Acc: 0.7446
0m 11s
Epoch 17/199
----------
LR 0.001
train Loss: 0.1944 Acc: 0.9341
val Loss: 0.8532 Acc: 0.7570
0m 11s
Epoch 18/199
----------
LR 0.001
train Loss: 0.1870 Acc: 0.9369
val Loss: 0.9568 Acc: 0.7407
0m 11s
Epoch 19/199
----------
LR 0.001
train Loss: 0.1780 Acc: 0.9398
val Loss: 0.8721 Acc: 0.7578
0m 11s
Epoch 20/199
----------
LR 0.001
train Loss: 0.1724 Acc: 0.9417
val Loss: 0.8817 Acc: 0.7572
0m 11s
Epoch 21/199
----------
LR 0.001
train Loss: 0.1656 Acc: 0.9439
val Loss: 0.9014 Acc: 0.7557
0m 11s
Epoch 22/199
----------
LR 0.001
train Loss: 0.1606 Acc: 0.9456
val Loss: 0.8830 Acc: 0.7633
saving best model
best_acc 0.7633
Model saved to results/cifar10/2021-12-01-19-19-49__resnet34_6839
0m 12s
Epoch 23/199
----------
LR 0.001
train Loss: 0.1569 Acc: 0.9467
val Loss: 0.9038 Acc: 0.7573
0m 11s
Epoch 24/199
----------
LR 0.001
train Loss: 0.1580 Acc: 0.9456
val Loss: 0.9615 Acc: 0.7514
0m 11s
Epoch 25/199
----------
LR 0.001
train Loss: 0.1552 Acc: 0.9468
val Loss: 0.9297 Acc: 0.7548
0m 11s
Epoch 26/199
----------
LR 0.001
train Loss: 0.1479 Acc: 0.9489
val Loss: 0.9401 Acc: 0.7557
0m 11s
Epoch 27/199
----------
LR 0.001
train Loss: 0.1504 Acc: 0.9489
val Loss: 0.8919 Acc: 0.7639
saving best model
best_acc 0.7639
Model saved to results/cifar10/2021-12-01-19-19-49__resnet34_6839
0m 12s
Epoch 28/199
----------
LR 0.001
train Loss: 0.1351 Acc: 0.9539
val Loss: 0.9765 Acc: 0.7418
0m 11s
Epoch 29/199
----------
LR 0.001
train Loss: 0.1444 Acc: 0.9498
val Loss: 0.9622 Acc: 0.7580
0m 11s
Epoch 30/199
----------
LR 0.001
train Loss: 0.1356 Acc: 0.9530
val Loss: 0.9057 Acc: 0.7574
0m 11s
Epoch 31/199
----------
LR 0.001
train Loss: 0.1378 Acc: 0.9529
val Loss: 0.9629 Acc: 0.7535
0m 11s
Epoch 32/199
----------
LR 0.001
train Loss: 0.1376 Acc: 0.9535
val Loss: 0.9163 Acc: 0.7618
0m 11s
Epoch 33/199
----------
LR 0.001
train Loss: 0.1335 Acc: 0.9541
val Loss: 0.9170 Acc: 0.7591
0m 11s
Epoch 34/199
----------
LR 0.001
train Loss: 0.1304 Acc: 0.9549
val Loss: 1.0169 Acc: 0.7411
0m 11s
Epoch 35/199
----------
LR 0.001
train Loss: 0.1306 Acc: 0.9552
val Loss: 0.9788 Acc: 0.7512
0m 11s
Epoch 36/199
----------
LR 0.001
train Loss: 0.1309 Acc: 0.9553
val Loss: 0.9610 Acc: 0.7577
0m 11s
Epoch 37/199
----------
LR 0.001
train Loss: 0.1257 Acc: 0.9569
val Loss: 0.9951 Acc: 0.7510
0m 11s
Epoch 38/199
----------
LR 0.001
train Loss: 0.1311 Acc: 0.9559
val Loss: 0.9474 Acc: 0.7538
0m 11s
Epoch 39/199
----------
LR 0.001
train Loss: 0.1230 Acc: 0.9596
val Loss: 0.9627 Acc: 0.7649
saving best model
best_acc 0.7649
Model saved to results/cifar10/2021-12-01-19-19-49__resnet34_6839
0m 12s
Epoch 40/199
----------
LR 0.001
train Loss: 0.1248 Acc: 0.9576
val Loss: 0.9865 Acc: 0.7542
0m 11s
Epoch 41/199
----------
LR 0.001
train Loss: 0.1303 Acc: 0.9566
val Loss: 0.9656 Acc: 0.7517
0m 11s
Epoch 42/199
----------
LR 0.001
train Loss: 0.1223 Acc: 0.9583
val Loss: 0.9522 Acc: 0.7526
0m 11s
Epoch 43/199
----------
LR 0.001
train Loss: 0.1218 Acc: 0.9585
val Loss: 0.9762 Acc: 0.7497
0m 11s
Epoch 44/199
----------
LR 0.001
train Loss: 0.1221 Acc: 0.9586
val Loss: 1.0308 Acc: 0.7489
0m 11s
Epoch 45/199
----------
LR 0.001
train Loss: 0.1228 Acc: 0.9587
val Loss: 1.0254 Acc: 0.7405
0m 11s
Epoch 46/199
----------
LR 0.001
train Loss: 0.1183 Acc: 0.9605
val Loss: 0.8983 Acc: 0.7705
saving best model
best_acc 0.7705
Model saved to results/cifar10/2021-12-01-19-19-49__resnet34_6839
0m 12s
Epoch 47/199
----------
LR 0.001
train Loss: 0.1204 Acc: 0.9595
val Loss: 0.9883 Acc: 0.7506
0m 11s
Epoch 48/199
----------
LR 0.001
train Loss: 0.1180 Acc: 0.9605
val Loss: 0.9657 Acc: 0.7613
0m 11s
Epoch 49/199
----------
LR 0.001
train Loss: 0.1167 Acc: 0.9600
val Loss: 0.9900 Acc: 0.7500
0m 11s
Epoch 50/199
----------
LR 0.001
train Loss: 0.1217 Acc: 0.9592
val Loss: 0.9736 Acc: 0.7589
0m 11s
Epoch 51/199
----------
LR 0.001
train Loss: 0.1141 Acc: 0.9628
val Loss: 0.9874 Acc: 0.7503
0m 11s
Epoch 52/199
----------
LR 0.001
train Loss: 0.1140 Acc: 0.9604
val Loss: 0.9891 Acc: 0.7520
0m 11s
Epoch 53/199
----------
LR 0.001
train Loss: 0.1156 Acc: 0.9617
val Loss: 0.9506 Acc: 0.7519
0m 11s
Epoch 54/199
----------
LR 0.001
train Loss: 0.1117 Acc: 0.9620
val Loss: 0.9597 Acc: 0.7589
0m 11s
Epoch 55/199
----------
LR 0.001
train Loss: 0.1126 Acc: 0.9620
val Loss: 0.9432 Acc: 0.7563
0m 11s
Epoch 56/199
----------
LR 0.001
train Loss: 0.1139 Acc: 0.9616
val Loss: 1.0127 Acc: 0.7453
0m 11s
Epoch 57/199
----------
LR 0.001
train Loss: 0.1155 Acc: 0.9608
val Loss: 0.9878 Acc: 0.7531
0m 11s
Epoch 58/199
----------
LR 0.001
train Loss: 0.1125 Acc: 0.9631
val Loss: 0.9723 Acc: 0.7522
0m 11s
Epoch 59/199
----------
LR 0.001
train Loss: 0.1127 Acc: 0.9626
val Loss: 0.9366 Acc: 0.7640
0m 11s
Epoch 60/199
----------
LR 0.0002
train Loss: 0.0395 Acc: 0.9882
val Loss: 0.8957 Acc: 0.7861
saving best model
best_acc 0.7861
Model saved to results/cifar10/2021-12-01-19-19-49__resnet34_6839
0m 12s
Epoch 61/199
----------
LR 0.0002
train Loss: 0.0091 Acc: 0.9989
val Loss: 0.9210 Acc: 0.7898
saving best model
best_acc 0.7898
Model saved to results/cifar10/2021-12-01-19-19-49__resnet34_6839
0m 12s
Epoch 62/199
----------
LR 0.0002
train Loss: 0.0050 Acc: 0.9997
val Loss: 0.9504 Acc: 0.7895
0m 11s
Epoch 63/199
----------
LR 0.0002
train Loss: 0.0034 Acc: 0.9999
val Loss: 0.9813 Acc: 0.7885
0m 11s
Epoch 64/199
----------
LR 0.0002
train Loss: 0.0024 Acc: 1.0000
val Loss: 1.0169 Acc: 0.7893
0m 11s
Epoch 65/199
----------
LR 0.0002
train Loss: 0.0020 Acc: 0.9999
val Loss: 1.0435 Acc: 0.7879
0m 11s
Epoch 66/199
----------
LR 0.0002
train Loss: 0.0019 Acc: 0.9999
val Loss: 1.0737 Acc: 0.7847
0m 11s
Epoch 67/199
----------
LR 0.0002
train Loss: 0.0111 Acc: 0.9971
val Loss: 1.0795 Acc: 0.7753
0m 11s
Epoch 68/199
----------
LR 0.0002
train Loss: 0.0191 Acc: 0.9941
val Loss: 1.0866 Acc: 0.7785
0m 11s
Epoch 69/199
----------
LR 0.0002
train Loss: 0.0097 Acc: 0.9974
val Loss: 1.1012 Acc: 0.7783
0m 11s
Epoch 70/199
----------
LR 0.0002
train Loss: 0.0082 Acc: 0.9977
val Loss: 1.1574 Acc: 0.7751
0m 11s
Epoch 71/199
----------
LR 0.0002
train Loss: 0.0088 Acc: 0.9977
val Loss: 1.1712 Acc: 0.7716
0m 11s
Epoch 72/199
----------
LR 0.0002
train Loss: 0.0134 Acc: 0.9959
val Loss: 1.1556 Acc: 0.7712
0m 11s
Epoch 73/199
----------
LR 0.0002
train Loss: 0.0141 Acc: 0.9958
val Loss: 1.1440 Acc: 0.7712
0m 11s
Epoch 74/199
----------
LR 0.0002
train Loss: 0.0095 Acc: 0.9973
val Loss: 1.1416 Acc: 0.7765
0m 11s
Epoch 75/199
----------
LR 0.0002
train Loss: 0.0062 Acc: 0.9984
val Loss: 1.1483 Acc: 0.7771
0m 11s
Epoch 76/199
----------
LR 0.0002
train Loss: 0.0107 Acc: 0.9968
val Loss: 1.1551 Acc: 0.7717
0m 11s
Epoch 77/199
----------
LR 0.0002
train Loss: 0.0143 Acc: 0.9958
val Loss: 1.1606 Acc: 0.7734
0m 11s
Epoch 78/199
----------
LR 0.0002
train Loss: 0.0136 Acc: 0.9958
val Loss: 1.1561 Acc: 0.7716
0m 11s
Epoch 79/199
----------
LR 0.0002
train Loss: 0.0090 Acc: 0.9974
val Loss: 1.1722 Acc: 0.7717
0m 11s
Epoch 80/199
----------
LR 0.0002
train Loss: 0.0118 Acc: 0.9970
val Loss: 1.1799 Acc: 0.7716
0m 11s
Epoch 81/199
----------
LR 0.0002
train Loss: 0.0139 Acc: 0.9959
val Loss: 1.1922 Acc: 0.7690
0m 11s
Epoch 82/199
----------
LR 0.0002
train Loss: 0.0125 Acc: 0.9964
val Loss: 1.1955 Acc: 0.7698
0m 11s
Epoch 83/199
----------
LR 0.0002
train Loss: 0.0117 Acc: 0.9967
val Loss: 1.1969 Acc: 0.7678
0m 11s
Epoch 84/199
----------
LR 0.0002
train Loss: 0.0104 Acc: 0.9972
val Loss: 1.1914 Acc: 0.7709
0m 11s
Epoch 85/199
----------
LR 0.0002
train Loss: 0.0134 Acc: 0.9961
val Loss: 1.2104 Acc: 0.7678
0m 11s
Epoch 86/199
----------
LR 0.0002
train Loss: 0.0093 Acc: 0.9975
val Loss: 1.2020 Acc: 0.7734
0m 11s
Epoch 87/199
----------
LR 0.0002
train Loss: 0.0146 Acc: 0.9956
val Loss: 1.1744 Acc: 0.7729
0m 11s
Epoch 88/199
----------
LR 0.0002
train Loss: 0.0149 Acc: 0.9955
val Loss: 1.1721 Acc: 0.7721
0m 11s
Epoch 89/199
----------
LR 0.0002
train Loss: 0.0142 Acc: 0.9960
val Loss: 1.1785 Acc: 0.7726
0m 11s
Epoch 90/199
----------
LR 0.0002
train Loss: 0.0097 Acc: 0.9974
val Loss: 1.2023 Acc: 0.7706
0m 11s
Epoch 91/199
----------
LR 0.0002
train Loss: 0.0140 Acc: 0.9958
val Loss: 1.1750 Acc: 0.7727
0m 11s
Epoch 92/199
----------
LR 0.0002
train Loss: 0.0127 Acc: 0.9964
val Loss: 1.1873 Acc: 0.7726
0m 11s
Epoch 93/199
----------
LR 0.0002
train Loss: 0.0146 Acc: 0.9955
val Loss: 1.2301 Acc: 0.7655
0m 11s
Epoch 94/199
----------
LR 0.0002
train Loss: 0.0137 Acc: 0.9961
val Loss: 1.1883 Acc: 0.7715
0m 11s
Epoch 95/199
----------
LR 0.0002
train Loss: 0.0107 Acc: 0.9969
val Loss: 1.1926 Acc: 0.7703
0m 11s
Epoch 96/199
----------
LR 0.0002
train Loss: 0.0135 Acc: 0.9962
val Loss: 1.2132 Acc: 0.7718
0m 11s
Epoch 97/199
----------
LR 0.0002
train Loss: 0.0135 Acc: 0.9964
val Loss: 1.2015 Acc: 0.7715
0m 11s
Epoch 98/199
----------
LR 0.0002
train Loss: 0.0142 Acc: 0.9958
val Loss: 1.2005 Acc: 0.7691
0m 11s
Epoch 99/199
----------
LR 0.0002
train Loss: 0.0150 Acc: 0.9954
val Loss: 1.1892 Acc: 0.7746
0m 11s
Epoch 100/199
----------
LR 0.0002
train Loss: 0.0135 Acc: 0.9959
val Loss: 1.2503 Acc: 0.7640
0m 11s
Epoch 101/199
----------
LR 0.0002
train Loss: 0.0134 Acc: 0.9960
val Loss: 1.1950 Acc: 0.7671
0m 11s
Epoch 102/199
----------
LR 0.0002
train Loss: 0.0150 Acc: 0.9958
val Loss: 1.1771 Acc: 0.7764
0m 11s
Epoch 103/199
----------
LR 0.0002
train Loss: 0.0135 Acc: 0.9963
val Loss: 1.1950 Acc: 0.7705
0m 11s
Epoch 104/199
----------
LR 0.0002
train Loss: 0.0123 Acc: 0.9965
val Loss: 1.2161 Acc: 0.7708
0m 11s
Epoch 105/199
----------
LR 0.0002
train Loss: 0.0147 Acc: 0.9955
val Loss: 1.1894 Acc: 0.7700
0m 11s
Epoch 106/199
----------
LR 0.0002
train Loss: 0.0164 Acc: 0.9950
val Loss: 1.2286 Acc: 0.7638
0m 11s
Epoch 107/199
----------
LR 0.0002
train Loss: 0.0158 Acc: 0.9952
val Loss: 1.2061 Acc: 0.7671
0m 11s
Epoch 108/199
----------
LR 0.0002
train Loss: 0.0123 Acc: 0.9966
val Loss: 1.2192 Acc: 0.7679
0m 11s
Epoch 109/199
----------
LR 0.0002
train Loss: 0.0144 Acc: 0.9959
val Loss: 1.1949 Acc: 0.7710
0m 11s
Epoch 110/199
----------
LR 0.0002
train Loss: 0.0169 Acc: 0.9949
val Loss: 1.1745 Acc: 0.7735
0m 11s
Epoch 111/199
----------
LR 0.0002
train Loss: 0.0110 Acc: 0.9971
val Loss: 1.2515 Acc: 0.7724
0m 11s
Epoch 112/199
----------
LR 0.0002
train Loss: 0.0176 Acc: 0.9947
val Loss: 1.1802 Acc: 0.7721
0m 11s
Epoch 113/199
----------
LR 0.0002
train Loss: 0.0158 Acc: 0.9954
val Loss: 1.1857 Acc: 0.7744
0m 11s
Epoch 114/199
----------
LR 0.0002
train Loss: 0.0151 Acc: 0.9952
val Loss: 1.2426 Acc: 0.7712
0m 11s
Epoch 115/199
----------
LR 0.0002
train Loss: 0.0150 Acc: 0.9955
val Loss: 1.2269 Acc: 0.7716
0m 11s
Epoch 116/199
----------
LR 0.0002
train Loss: 0.0123 Acc: 0.9965
val Loss: 1.2734 Acc: 0.7664
0m 11s
Epoch 117/199
----------
LR 0.0002
train Loss: 0.0179 Acc: 0.9947
val Loss: 1.2268 Acc: 0.7704
0m 11s
Epoch 118/199
----------
LR 0.0002
train Loss: 0.0191 Acc: 0.9940
val Loss: 1.1919 Acc: 0.7661
0m 11s
Epoch 119/199
----------
LR 0.0002
train Loss: 0.0132 Acc: 0.9963
val Loss: 1.2366 Acc: 0.7648
0m 11s
Epoch 120/199
----------
LR 4e-05
train Loss: 0.0063 Acc: 0.9984
val Loss: 1.2052 Acc: 0.7708
0m 11s
Epoch 121/199
----------
LR 4e-05
train Loss: 0.0023 Acc: 0.9999
val Loss: 1.2091 Acc: 0.7710
0m 11s
Epoch 122/199
----------
LR 4e-05
train Loss: 0.0018 Acc: 0.9999
val Loss: 1.2086 Acc: 0.7734
0m 11s
Epoch 123/199
----------
LR 4e-05
train Loss: 0.0018 Acc: 0.9999
val Loss: 1.2108 Acc: 0.7745
0m 11s
Epoch 124/199
----------
LR 4e-05
train Loss: 0.0016 Acc: 0.9999
val Loss: 1.2127 Acc: 0.7730
0m 11s
Epoch 125/199
----------
LR 4e-05
train Loss: 0.0016 Acc: 1.0000
val Loss: 1.2140 Acc: 0.7742
0m 11s
Epoch 126/199
----------
LR 4e-05
train Loss: 0.0014 Acc: 1.0000
val Loss: 1.2182 Acc: 0.7769
0m 11s
Epoch 127/199
----------
LR 4e-05
train Loss: 0.0014 Acc: 1.0000
val Loss: 1.2148 Acc: 0.7759
0m 11s
Epoch 128/199
----------
LR 4e-05
train Loss: 0.0014 Acc: 1.0000
val Loss: 1.2112 Acc: 0.7745
0m 11s
Epoch 129/199
----------
LR 4e-05
train Loss: 0.0013 Acc: 1.0000
val Loss: 1.2104 Acc: 0.7759
0m 11s
Epoch 130/199
----------
LR 4e-05
train Loss: 0.0015 Acc: 1.0000
val Loss: 1.2059 Acc: 0.7764
0m 11s
Epoch 131/199
----------
LR 4e-05
train Loss: 0.0015 Acc: 0.9999
val Loss: 1.2219 Acc: 0.7779
0m 11s
Epoch 132/199
----------
LR 4e-05
train Loss: 0.0015 Acc: 1.0000
val Loss: 1.2327 Acc: 0.7781
0m 11s
Epoch 133/199
----------
LR 4e-05
train Loss: 0.0014 Acc: 1.0000
val Loss: 1.2514 Acc: 0.7783
0m 11s
Epoch 134/199
----------
LR 4e-05
train Loss: 0.0014 Acc: 1.0000
val Loss: 1.2557 Acc: 0.7752
0m 11s
Epoch 135/199
----------
LR 4e-05
train Loss: 0.0023 Acc: 0.9998
val Loss: 1.3271 Acc: 0.7679
0m 11s
Epoch 136/199
----------
LR 4e-05
train Loss: 0.0019 Acc: 0.9999
val Loss: 1.2994 Acc: 0.7734
0m 11s
Epoch 137/199
----------
LR 4e-05
train Loss: 0.0017 Acc: 0.9999
val Loss: 1.2866 Acc: 0.7762
0m 11s
Epoch 138/199
----------
LR 4e-05
train Loss: 0.0015 Acc: 1.0000
val Loss: 1.2836 Acc: 0.7774
0m 11s
Epoch 139/199
----------
LR 4e-05
train Loss: 0.0019 Acc: 0.9998
val Loss: 1.2883 Acc: 0.7771
0m 11s
Epoch 140/199
----------
LR 4e-05
train Loss: 0.0023 Acc: 0.9997
val Loss: 1.3037 Acc: 0.7718
0m 11s
Epoch 141/199
----------
LR 4e-05
train Loss: 0.0017 Acc: 0.9999
val Loss: 1.3099 Acc: 0.7719
0m 11s
Epoch 142/199
----------
LR 4e-05
train Loss: 0.0020 Acc: 0.9998
val Loss: 1.3176 Acc: 0.7703
0m 11s
Epoch 143/199
----------
LR 4e-05
train Loss: 0.0018 Acc: 0.9998
val Loss: 1.3327 Acc: 0.7695
0m 11s
Epoch 144/199
----------
LR 4e-05
train Loss: 0.0018 Acc: 0.9999
val Loss: 1.3253 Acc: 0.7716
0m 11s
Epoch 145/199
----------
LR 4e-05
train Loss: 0.0019 Acc: 0.9999
val Loss: 1.3203 Acc: 0.7717
0m 11s
Epoch 146/199
----------
LR 4e-05
train Loss: 0.0018 Acc: 0.9998
val Loss: 1.3319 Acc: 0.7709
0m 11s
Epoch 147/199
----------
LR 4e-05
train Loss: 0.0017 Acc: 0.9999
val Loss: 1.3200 Acc: 0.7733
0m 11s
Epoch 148/199
----------
LR 4e-05
train Loss: 0.0017 Acc: 0.9999
val Loss: 1.3423 Acc: 0.7729
0m 11s
Epoch 149/199
----------
LR 4e-05
train Loss: 0.0024 Acc: 0.9997
val Loss: 1.3276 Acc: 0.7738
0m 11s
Epoch 150/199
----------
LR 4e-05
train Loss: 0.0022 Acc: 0.9997
val Loss: 1.3260 Acc: 0.7739
0m 11s
Epoch 151/199
----------
LR 4e-05
train Loss: 0.0021 Acc: 0.9997
val Loss: 1.3317 Acc: 0.7770
0m 11s
Epoch 152/199
----------
LR 4e-05
train Loss: 0.0021 Acc: 0.9997
val Loss: 1.3283 Acc: 0.7781
0m 11s
Epoch 153/199
----------
LR 4e-05
train Loss: 0.0020 Acc: 0.9998
val Loss: 1.3199 Acc: 0.7775
0m 11s
Epoch 154/199
----------
LR 4e-05
train Loss: 0.0017 Acc: 0.9999
val Loss: 1.3253 Acc: 0.7793
0m 11s
Epoch 155/199
----------
LR 4e-05
train Loss: 0.0018 Acc: 0.9998
val Loss: 1.3334 Acc: 0.7745
0m 11s
Epoch 156/199
----------
LR 4e-05
train Loss: 0.0022 Acc: 0.9996
val Loss: 1.3530 Acc: 0.7767
0m 11s
Epoch 157/199
----------
LR 4e-05
train Loss: 0.0025 Acc: 0.9996
val Loss: 1.3450 Acc: 0.7749
0m 11s
Epoch 158/199
----------
LR 4e-05
train Loss: 0.0022 Acc: 0.9998
val Loss: 1.3468 Acc: 0.7753
0m 11s
Epoch 159/199
----------
LR 4e-05
train Loss: 0.0016 Acc: 1.0000
val Loss: 1.3552 Acc: 0.7761
0m 11s
Epoch 160/199
----------
LR 4e-05
train Loss: 0.0016 Acc: 0.9999
val Loss: 1.3411 Acc: 0.7767
0m 11s
Epoch 161/199
----------
LR 4e-05
train Loss: 0.0020 Acc: 0.9998
val Loss: 1.3700 Acc: 0.7724
0m 11s
Epoch 162/199
----------
LR 4e-05
train Loss: 0.0021 Acc: 0.9998
val Loss: 1.3664 Acc: 0.7750
0m 11s
Epoch 163/199
----------
LR 4e-05
train Loss: 0.0023 Acc: 0.9997
val Loss: 1.3577 Acc: 0.7730
0m 11s
Epoch 164/199
----------
LR 4e-05
train Loss: 0.0019 Acc: 0.9998
val Loss: 1.3664 Acc: 0.7740
0m 11s
Epoch 165/199
----------
LR 4e-05
train Loss: 0.0024 Acc: 0.9998
val Loss: 1.3540 Acc: 0.7738
0m 11s
Epoch 166/199
----------
LR 4e-05
train Loss: 0.0022 Acc: 0.9997
val Loss: 1.3534 Acc: 0.7733
0m 11s
Epoch 167/199
----------
LR 4e-05
train Loss: 0.0019 Acc: 0.9998
val Loss: 1.3612 Acc: 0.7745
0m 11s
Epoch 168/199
----------
LR 4e-05
train Loss: 0.0019 Acc: 0.9998
val Loss: 1.3465 Acc: 0.7769
0m 11s
Epoch 169/199
----------
LR 4e-05
train Loss: 0.0015 Acc: 1.0000
val Loss: 1.3509 Acc: 0.7758
0m 11s
Epoch 170/199
----------
LR 4e-05
train Loss: 0.0026 Acc: 0.9996
val Loss: 1.3829 Acc: 0.7722
0m 11s
Epoch 171/199
----------
LR 4e-05
train Loss: 0.0028 Acc: 0.9995
val Loss: 1.3596 Acc: 0.7765
0m 11s
Epoch 172/199
----------
LR 4e-05
train Loss: 0.0023 Acc: 0.9997
val Loss: 1.3774 Acc: 0.7710
0m 11s
Epoch 173/199
----------
LR 4e-05
train Loss: 0.0023 Acc: 0.9998
val Loss: 1.3770 Acc: 0.7742
0m 11s
Epoch 174/199
----------
LR 4e-05
train Loss: 0.0021 Acc: 0.9997
val Loss: 1.3863 Acc: 0.7735
0m 11s
Epoch 175/199
----------
LR 4e-05
train Loss: 0.0017 Acc: 0.9999
val Loss: 1.3808 Acc: 0.7738
0m 11s
Epoch 176/199
----------
LR 4e-05
train Loss: 0.0019 Acc: 0.9999
val Loss: 1.3766 Acc: 0.7757
0m 11s
Epoch 177/199
----------
LR 4e-05
train Loss: 0.0020 Acc: 0.9997
val Loss: 1.3828 Acc: 0.7741
0m 11s
Epoch 178/199
----------
LR 4e-05
train Loss: 0.0025 Acc: 0.9996
val Loss: 1.4070 Acc: 0.7736
0m 11s
Epoch 179/199
----------
LR 4e-05
train Loss: 0.0025 Acc: 0.9996
val Loss: 1.3890 Acc: 0.7754
0m 11s
Epoch 180/199
----------
LR 8.000000000000001e-06
train Loss: 0.0018 Acc: 0.9999
val Loss: 1.3869 Acc: 0.7771
0m 11s
Epoch 181/199
----------
LR 8.000000000000001e-06
train Loss: 0.0016 Acc: 0.9999
val Loss: 1.3770 Acc: 0.7778
0m 11s
Epoch 182/199
----------
LR 8.000000000000001e-06
train Loss: 0.0013 Acc: 1.0000
val Loss: 1.3816 Acc: 0.7787
0m 11s
Epoch 183/199
----------
LR 8.000000000000001e-06
train Loss: 0.0014 Acc: 0.9999
val Loss: 1.3804 Acc: 0.7784
0m 11s
Epoch 184/199
----------
LR 8.000000000000001e-06
train Loss: 0.0013 Acc: 1.0000
val Loss: 1.3776 Acc: 0.7785
0m 11s
Epoch 185/199
----------
LR 8.000000000000001e-06
train Loss: 0.0013 Acc: 1.0000
val Loss: 1.3700 Acc: 0.7792
0m 11s
Epoch 186/199
----------
LR 8.000000000000001e-06
train Loss: 0.0013 Acc: 1.0000
val Loss: 1.3763 Acc: 0.7782
0m 11s
Epoch 187/199
----------
LR 8.000000000000001e-06
train Loss: 0.0013 Acc: 1.0000
val Loss: 1.3724 Acc: 0.7779
0m 11s
Epoch 188/199
----------
LR 8.000000000000001e-06
train Loss: 0.0013 Acc: 1.0000
val Loss: 1.3661 Acc: 0.7784
0m 11s
Epoch 189/199
----------
LR 8.000000000000001e-06
train Loss: 0.0013 Acc: 1.0000
val Loss: 1.3620 Acc: 0.7791
0m 11s
Epoch 190/199
----------
LR 8.000000000000001e-06
train Loss: 0.0013 Acc: 1.0000
val Loss: 1.3621 Acc: 0.7773
0m 11s
Epoch 191/199
----------
LR 8.000000000000001e-06
train Loss: 0.0013 Acc: 1.0000
val Loss: 1.3521 Acc: 0.7792
0m 11s
Epoch 192/199
----------
LR 8.000000000000001e-06
train Loss: 0.0014 Acc: 0.9999
val Loss: 1.3527 Acc: 0.7781
0m 11s
Epoch 193/199
----------
LR 8.000000000000001e-06
train Loss: 0.0013 Acc: 1.0000
val Loss: 1.3470 Acc: 0.7795
0m 11s
Epoch 194/199
----------
LR 8.000000000000001e-06
train Loss: 0.0013 Acc: 1.0000
val Loss: 1.3497 Acc: 0.7793
0m 11s
Epoch 195/199
----------
LR 8.000000000000001e-06
train Loss: 0.0013 Acc: 1.0000
val Loss: 1.3505 Acc: 0.7785
0m 11s
Epoch 196/199
----------
LR 8.000000000000001e-06
train Loss: 0.0013 Acc: 1.0000
val Loss: 1.3425 Acc: 0.7781
0m 11s
Epoch 197/199
----------
LR 8.000000000000001e-06
train Loss: 0.0014 Acc: 1.0000
val Loss: 1.3444 Acc: 0.7787
0m 11s
Epoch 198/199
----------
LR 8.000000000000001e-06
train Loss: 0.0013 Acc: 1.0000
val Loss: 1.3497 Acc: 0.7785
0m 11s
Epoch 199/199
----------
LR 8.000000000000001e-06
train Loss: 0.0013 Acc: 1.0000
val Loss: 1.3500 Acc: 0.7787
0m 11s
Best val acc: 0.789800
Model :ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
Optimizer : Adamomentum
Epoch 0/199
----------
LR 0.001
train Loss: 1.8078 Acc: 0.3814
val Loss: 4.2166 Acc: 0.3820
saving best model
best_acc 0.382
Model saved to results/cifar10/2021-12-01-19-19-49__resnet34_6839
0m 11s
Epoch 1/199
----------
LR 0.001
train Loss: 1.6163 Acc: 0.4537
val Loss: 1.4587 Acc: 0.4882
saving best model
best_acc 0.4882
Model saved to results/cifar10/2021-12-01-19-19-49__resnet34_6839
0m 12s
Epoch 2/199
----------
LR 0.001
train Loss: 1.2930 Acc: 0.5533
val Loss: 1.1927 Acc: 0.5745
saving best model
best_acc 0.5745
Model saved to results/cifar10/2021-12-01-19-19-49__resnet34_6839
0m 12s
Epoch 3/199
----------
LR 0.001
train Loss: 1.0930 Acc: 0.6171
val Loss: 1.0519 Acc: 0.6422
saving best model
best_acc 0.6422
Model saved to results/cifar10/2021-12-01-19-19-49__resnet34_6839
0m 12s
Epoch 4/199
----------
LR 0.001
train Loss: 0.9189 Acc: 0.6809
val Loss: 1.0393 Acc: 0.6556
saving best model
best_acc 0.6556
Model saved to results/cifar10/2021-12-01-19-19-49__resnet34_6839
0m 12s
Epoch 5/199
----------
LR 0.001
train Loss: 0.7972 Acc: 0.7225
val Loss: 0.9633 Acc: 0.6878
saving best model
best_acc 0.6878
Model saved to results/cifar10/2021-12-01-19-19-49__resnet34_6839
0m 12s
Epoch 6/199
----------
LR 0.001
train Loss: 0.7302 Acc: 0.7476
val Loss: 0.8405 Acc: 0.7095
saving best model
best_acc 0.7095
Model saved to results/cifar10/2021-12-01-19-19-49__resnet34_6839
0m 12s
Epoch 7/199
----------
LR 0.001
train Loss: 0.6393 Acc: 0.7792
val Loss: 0.8932 Acc: 0.6986
0m 11s
Epoch 8/199
----------
LR 0.001
train Loss: 0.5538 Acc: 0.8076
val Loss: 0.8001 Acc: 0.7299
saving best model
best_acc 0.7299
Model saved to results/cifar10/2021-12-01-19-19-49__resnet34_6839
0m 12s
Epoch 9/199
----------
LR 0.001
train Loss: 0.4782 Acc: 0.8325
val Loss: 0.8585 Acc: 0.7202
0m 11s
Epoch 10/199
----------
LR 0.001
train Loss: 0.4212 Acc: 0.8519
val Loss: 0.8304 Acc: 0.7328
saving best model
best_acc 0.7328
Model saved to results/cifar10/2021-12-01-19-19-49__resnet34_6839
0m 12s
Epoch 11/199
----------
LR 0.001
train Loss: 0.3787 Acc: 0.8669
val Loss: 0.8587 Acc: 0.7331
saving best model
best_acc 0.7331
Model saved to results/cifar10/2021-12-01-19-19-49__resnet34_6839
0m 12s
Epoch 12/199
----------
LR 0.001
train Loss: 0.3336 Acc: 0.8830
val Loss: 0.8799 Acc: 0.7331
0m 11s
Epoch 13/199
----------
LR 0.001
train Loss: 0.3002 Acc: 0.8957
val Loss: 0.8937 Acc: 0.7373
saving best model
best_acc 0.7373
Model saved to results/cifar10/2021-12-01-19-19-49__resnet34_6839
0m 12s
Epoch 14/199
----------
LR 0.001
train Loss: 0.2652 Acc: 0.9070
val Loss: 0.8807 Acc: 0.7424
saving best model
best_acc 0.7424
Model saved to results/cifar10/2021-12-01-19-19-49__resnet34_6839
0m 12s
Epoch 15/199
----------
LR 0.001
train Loss: 0.2392 Acc: 0.9175
val Loss: 0.8974 Acc: 0.7474
saving best model
best_acc 0.7474
Model saved to results/cifar10/2021-12-01-19-19-49__resnet34_6839
0m 12s
Epoch 16/199
----------
LR 0.001
train Loss: 0.2274 Acc: 0.9208
val Loss: 0.9952 Acc: 0.7232
0m 11s
Epoch 17/199
----------
LR 0.001
train Loss: 0.2084 Acc: 0.9273
val Loss: 1.0003 Acc: 0.7218
0m 11s
Epoch 18/199
----------
LR 0.001
train Loss: 0.1915 Acc: 0.9328
val Loss: 1.0031 Acc: 0.7316
0m 11s
Epoch 19/199
----------
LR 0.001
train Loss: 0.1928 Acc: 0.9329
val Loss: 1.0135 Acc: 0.7324
0m 11s
Epoch 20/199
----------
LR 0.001
train Loss: 0.1703 Acc: 0.9405
val Loss: 0.9543 Acc: 0.7448
0m 11s
Epoch 21/199
----------
LR 0.001
train Loss: 0.1724 Acc: 0.9388
val Loss: 1.0122 Acc: 0.7351
0m 11s
Epoch 22/199
----------
LR 0.001
train Loss: 0.1634 Acc: 0.9425
val Loss: 1.0023 Acc: 0.7414
0m 11s
Epoch 23/199
----------
LR 0.001
train Loss: 0.1585 Acc: 0.9445
val Loss: 1.0905 Acc: 0.7224
0m 11s
Epoch 24/199
----------
LR 0.001
train Loss: 0.1513 Acc: 0.9486
val Loss: 1.0678 Acc: 0.7244
0m 11s
Epoch 25/199
----------
LR 0.001
train Loss: 0.1500 Acc: 0.9481
val Loss: 1.0630 Acc: 0.7361
0m 11s
Epoch 26/199
----------
LR 0.001
train Loss: 0.1443 Acc: 0.9505
val Loss: 1.0254 Acc: 0.7351
0m 11s
Epoch 27/199
----------
LR 0.001
train Loss: 0.1433 Acc: 0.9508
val Loss: 1.0509 Acc: 0.7393
0m 11s
Epoch 28/199
----------
LR 0.001
train Loss: 0.1397 Acc: 0.9519
val Loss: 1.0518 Acc: 0.7332
0m 11s
Epoch 29/199
----------
LR 0.001
train Loss: 0.1397 Acc: 0.9518
val Loss: 0.9969 Acc: 0.7423
0m 11s
Epoch 30/199
----------
LR 0.001
train Loss: 0.1468 Acc: 0.9493
val Loss: 1.0305 Acc: 0.7348
0m 11s
Epoch 31/199
----------
LR 0.001
train Loss: 0.1241 Acc: 0.9581
val Loss: 1.0606 Acc: 0.7341
0m 11s
Epoch 32/199
----------
LR 0.001
train Loss: 0.1263 Acc: 0.9566
val Loss: 1.0396 Acc: 0.7342
0m 11s
Epoch 33/199
----------
LR 0.001
train Loss: 0.1320 Acc: 0.9541
val Loss: 1.0641 Acc: 0.7284
0m 11s
Epoch 34/199
----------
LR 0.001
train Loss: 0.1385 Acc: 0.9525
val Loss: 1.0459 Acc: 0.7360
0m 11s
Epoch 35/199
----------
LR 0.001
train Loss: 0.1332 Acc: 0.9549
val Loss: 1.0540 Acc: 0.7343
0m 11s
Epoch 36/199
----------
LR 0.001
train Loss: 0.1249 Acc: 0.9579
val Loss: 1.0607 Acc: 0.7399
0m 11s
Epoch 37/199
----------
LR 0.001
train Loss: 0.1271 Acc: 0.9557
val Loss: 0.9726 Acc: 0.7453
0m 11s
Epoch 38/199
----------
LR 0.001
train Loss: 0.1272 Acc: 0.9557
val Loss: 1.1265 Acc: 0.7259
0m 11s
Epoch 39/199
----------
LR 0.001
train Loss: 0.1281 Acc: 0.9560
val Loss: 1.0310 Acc: 0.7439
0m 11s
Epoch 40/199
----------
LR 0.001
train Loss: 0.1196 Acc: 0.9589
val Loss: 1.0759 Acc: 0.7396
0m 11s
Epoch 41/199
----------
LR 0.001
train Loss: 0.1268 Acc: 0.9566
val Loss: 1.0108 Acc: 0.7476
saving best model
best_acc 0.7476
Model saved to results/cifar10/2021-12-01-19-19-49__resnet34_6839
0m 12s
Epoch 42/199
----------
LR 0.001
train Loss: 0.1181 Acc: 0.9597
val Loss: 1.0165 Acc: 0.7445
0m 11s
Epoch 43/199
----------
LR 0.001
train Loss: 0.1168 Acc: 0.9601
val Loss: 1.0611 Acc: 0.7423
0m 11s
Epoch 44/199
----------
LR 0.001
train Loss: 0.1226 Acc: 0.9582
val Loss: 0.9663 Acc: 0.7508
saving best model
best_acc 0.7508
Model saved to results/cifar10/2021-12-01-19-19-49__resnet34_6839
0m 12s
Epoch 45/199
----------
LR 0.001
train Loss: 0.1136 Acc: 0.9607
val Loss: 0.9870 Acc: 0.7519
saving best model
best_acc 0.7519
Model saved to results/cifar10/2021-12-01-19-19-49__resnet34_6839
0m 12s
Epoch 46/199
----------
LR 0.001
train Loss: 0.1194 Acc: 0.9597
val Loss: 1.0879 Acc: 0.7280
0m 11s
Epoch 47/199
----------
LR 0.001
train Loss: 0.1188 Acc: 0.9591
val Loss: 1.0516 Acc: 0.7387
0m 11s
Epoch 48/199
----------
LR 0.001
train Loss: 0.1230 Acc: 0.9584
val Loss: 1.0073 Acc: 0.7385
0m 11s
Epoch 49/199
----------
LR 0.001
train Loss: 0.1188 Acc: 0.9594
val Loss: 1.0379 Acc: 0.7442
0m 11s
Epoch 50/199
----------
LR 0.001
train Loss: 0.1197 Acc: 0.9590
val Loss: 1.0484 Acc: 0.7369
0m 11s
Epoch 51/199
----------
LR 0.001
train Loss: 0.1172 Acc: 0.9600
val Loss: 1.0484 Acc: 0.7400
0m 11s
Epoch 52/199
----------
LR 0.001
train Loss: 0.1158 Acc: 0.9600
val Loss: 1.0353 Acc: 0.7425
0m 11s
Epoch 53/199
----------
LR 0.001
train Loss: 0.1180 Acc: 0.9599
val Loss: 1.0210 Acc: 0.7367
0m 11s
Epoch 54/199
----------
LR 0.001
train Loss: 0.1120 Acc: 0.9620
val Loss: 1.0034 Acc: 0.7497
0m 11s
Epoch 55/199
----------
LR 0.001
train Loss: 0.1135 Acc: 0.9610
val Loss: 0.9813 Acc: 0.7513
0m 11s
Epoch 56/199
----------
LR 0.001
train Loss: 0.1184 Acc: 0.9585
val Loss: 1.0333 Acc: 0.7418
0m 11s
Epoch 57/199
----------
LR 0.001
train Loss: 0.1175 Acc: 0.9597
val Loss: 1.0698 Acc: 0.7347
0m 11s
Epoch 58/199
----------
LR 0.001
train Loss: 0.1101 Acc: 0.9625
val Loss: 1.0678 Acc: 0.7365
0m 11s
Epoch 59/199
----------
LR 0.001
train Loss: 0.1170 Acc: 0.9605
val Loss: 1.0118 Acc: 0.7522
saving best model
best_acc 0.7522
Model saved to results/cifar10/2021-12-01-19-19-49__resnet34_6839
0m 12s
Epoch 60/199
----------
LR 0.0002
train Loss: 0.0286 Acc: 0.9921
val Loss: 0.9068 Acc: 0.7808
saving best model
best_acc 0.7808
Model saved to results/cifar10/2021-12-01-19-19-49__resnet34_6839
0m 12s
Epoch 61/199
----------
LR 0.0002
train Loss: 0.0050 Acc: 0.9996
val Loss: 0.9330 Acc: 0.7831
saving best model
best_acc 0.7831
Model saved to results/cifar10/2021-12-01-19-19-49__resnet34_6839
0m 12s
Epoch 62/199
----------
LR 0.0002
train Loss: 0.0029 Acc: 0.9999
val Loss: 0.9501 Acc: 0.7837
saving best model
best_acc 0.7837
Model saved to results/cifar10/2021-12-01-19-19-49__resnet34_6839
0m 12s
Epoch 63/199
----------
LR 0.0002
train Loss: 0.0021 Acc: 1.0000
val Loss: 0.9611 Acc: 0.7837
0m 11s
Epoch 64/199
----------
LR 0.0002
train Loss: 0.0017 Acc: 1.0000
val Loss: 0.9716 Acc: 0.7868
saving best model
best_acc 0.7868
Model saved to results/cifar10/2021-12-01-19-19-49__resnet34_6839
0m 12s
Epoch 65/199
----------
LR 0.0002
train Loss: 0.0015 Acc: 1.0000
val Loss: 0.9792 Acc: 0.7854
0m 11s
Epoch 66/199
----------
LR 0.0002
train Loss: 0.0013 Acc: 1.0000
val Loss: 0.9789 Acc: 0.7851
0m 11s
Epoch 67/199
----------
LR 0.0002
train Loss: 0.0011 Acc: 1.0000
val Loss: 0.9810 Acc: 0.7868
0m 11s
Epoch 68/199
----------
LR 0.0002
train Loss: 0.0011 Acc: 1.0000
val Loss: 0.9880 Acc: 0.7861
0m 11s
Epoch 69/199
----------
LR 0.0002
train Loss: 0.0011 Acc: 1.0000
val Loss: 0.9795 Acc: 0.7883
saving best model
best_acc 0.7883
Model saved to results/cifar10/2021-12-01-19-19-49__resnet34_6839
0m 12s
Epoch 70/199
----------
LR 0.0002
train Loss: 0.0011 Acc: 1.0000
val Loss: 0.9798 Acc: 0.7871
0m 11s
Epoch 71/199
----------
LR 0.0002
train Loss: 0.0010 Acc: 1.0000
val Loss: 0.9774 Acc: 0.7871
0m 11s
Epoch 72/199
----------
LR 0.0002
train Loss: 0.0010 Acc: 1.0000
val Loss: 0.9730 Acc: 0.7879
0m 11s
Epoch 73/199
----------
LR 0.0002
train Loss: 0.0010 Acc: 1.0000
val Loss: 0.9737 Acc: 0.7882
0m 11s
Epoch 74/199
----------
LR 0.0002
train Loss: 0.0010 Acc: 1.0000
val Loss: 0.9770 Acc: 0.7888
saving best model
best_acc 0.7888
Model saved to results/cifar10/2021-12-01-19-19-49__resnet34_6839
0m 12s
Epoch 75/199
----------
LR 0.0002
train Loss: 0.0010 Acc: 1.0000
val Loss: 0.9779 Acc: 0.7877
0m 11s
Epoch 76/199
----------
LR 0.0002
train Loss: 0.0010 Acc: 1.0000
val Loss: 0.9726 Acc: 0.7873
0m 11s
Epoch 77/199
----------
LR 0.0002
train Loss: 0.0010 Acc: 1.0000
val Loss: 0.9787 Acc: 0.7884
0m 11s
Epoch 78/199
----------
LR 0.0002
train Loss: 0.0009 Acc: 1.0000
val Loss: 0.9722 Acc: 0.7902
saving best model
best_acc 0.7902
Model saved to results/cifar10/2021-12-01-19-19-49__resnet34_6839
0m 12s
Epoch 79/199
----------
LR 0.0002
train Loss: 0.0009 Acc: 1.0000
val Loss: 0.9743 Acc: 0.7878
0m 11s
Epoch 80/199
----------
LR 0.0002
train Loss: 0.0009 Acc: 1.0000
val Loss: 0.9734 Acc: 0.7876
0m 11s
Epoch 81/199
----------
LR 0.0002
train Loss: 0.0009 Acc: 1.0000
val Loss: 0.9713 Acc: 0.7893
0m 11s
Epoch 82/199
----------
LR 0.0002
train Loss: 0.0009 Acc: 1.0000
val Loss: 0.9689 Acc: 0.7890
0m 11s
Epoch 83/199
----------
LR 0.0002
train Loss: 0.0009 Acc: 1.0000
val Loss: 0.9738 Acc: 0.7893
0m 11s
Epoch 84/199
----------
LR 0.0002
train Loss: 0.0010 Acc: 1.0000
val Loss: 0.9717 Acc: 0.7906
saving best model
best_acc 0.7906
Model saved to results/cifar10/2021-12-01-19-19-49__resnet34_6839
0m 12s
Epoch 85/199
----------
LR 0.0002
train Loss: 0.0009 Acc: 1.0000
val Loss: 0.9766 Acc: 0.7881
0m 11s
Epoch 86/199
----------
LR 0.0002
train Loss: 0.0009 Acc: 1.0000
val Loss: 0.9782 Acc: 0.7894
0m 11s
Epoch 87/199
----------
LR 0.0002
train Loss: 0.0009 Acc: 1.0000
val Loss: 0.9775 Acc: 0.7890
0m 11s
Epoch 88/199
----------
LR 0.0002
train Loss: 0.0009 Acc: 1.0000
val Loss: 0.9830 Acc: 0.7904
0m 11s
Epoch 89/199
----------
LR 0.0002
train Loss: 0.0009 Acc: 1.0000
val Loss: 0.9815 Acc: 0.7898
0m 11s
Epoch 90/199
----------
LR 0.0002
train Loss: 0.0009 Acc: 1.0000
val Loss: 0.9824 Acc: 0.7901
0m 11s
Epoch 91/199
----------
LR 0.0002
train Loss: 0.0009 Acc: 1.0000
val Loss: 0.9872 Acc: 0.7908
saving best model
best_acc 0.7908
Model saved to results/cifar10/2021-12-01-19-19-49__resnet34_6839
0m 12s
Epoch 92/199
----------
LR 0.0002
train Loss: 0.0009 Acc: 1.0000
val Loss: 0.9900 Acc: 0.7903
0m 11s
Epoch 93/199
----------
LR 0.0002
train Loss: 0.0009 Acc: 1.0000
val Loss: 0.9911 Acc: 0.7899
0m 11s
Epoch 94/199
----------
LR 0.0002
train Loss: 0.0009 Acc: 1.0000
val Loss: 0.9900 Acc: 0.7899
0m 11s
Epoch 95/199
----------
LR 0.0002
train Loss: 0.0009 Acc: 1.0000
val Loss: 0.9939 Acc: 0.7902
0m 11s
Epoch 96/199
----------
LR 0.0002
train Loss: 0.0009 Acc: 1.0000
val Loss: 0.9992 Acc: 0.7905
0m 11s
Epoch 97/199
----------
LR 0.0002
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0025 Acc: 0.7901
0m 11s
Epoch 98/199
----------
LR 0.0002
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0084 Acc: 0.7899
0m 11s
Epoch 99/199
----------
LR 0.0002
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0097 Acc: 0.7886
0m 11s
Epoch 100/199
----------
LR 0.0002
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0134 Acc: 0.7886
0m 11s
Epoch 101/199
----------
LR 0.0002
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0124 Acc: 0.7889
0m 11s
Epoch 102/199
----------
LR 0.0002
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0123 Acc: 0.7904
0m 11s
Epoch 103/199
----------
LR 0.0002
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0199 Acc: 0.7891
0m 11s
Epoch 104/199
----------
LR 0.0002
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0212 Acc: 0.7900
0m 11s
Epoch 105/199
----------
LR 0.0002
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0255 Acc: 0.7883
0m 11s
Epoch 106/199
----------
LR 0.0002
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0233 Acc: 0.7891
0m 11s
Epoch 107/199
----------
LR 0.0002
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0251 Acc: 0.7876
0m 11s
Epoch 108/199
----------
LR 0.0002
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0302 Acc: 0.7890
0m 11s
Epoch 109/199
----------
LR 0.0002
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0309 Acc: 0.7890
0m 11s
Epoch 110/199
----------
LR 0.0002
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0330 Acc: 0.7914
saving best model
best_acc 0.7914
Model saved to results/cifar10/2021-12-01-19-19-49__resnet34_6839
0m 12s
Epoch 111/199
----------
LR 0.0002
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0322 Acc: 0.7905
0m 11s
Epoch 112/199
----------
LR 0.0002
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0397 Acc: 0.7896
0m 11s
Epoch 113/199
----------
LR 0.0002
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0443 Acc: 0.7895
0m 11s
Epoch 114/199
----------
LR 0.0002
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0410 Acc: 0.7895
0m 11s
Epoch 115/199
----------
LR 0.0002
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0476 Acc: 0.7891
0m 11s
Epoch 116/199
----------
LR 0.0002
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0413 Acc: 0.7901
0m 11s
Epoch 117/199
----------
LR 0.0002
train Loss: 0.0010 Acc: 1.0000
val Loss: 1.0544 Acc: 0.7917
saving best model
best_acc 0.7917
Model saved to results/cifar10/2021-12-01-19-19-49__resnet34_6839
0m 12s
Epoch 118/199
----------
LR 0.0002
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0598 Acc: 0.7889
0m 11s
Epoch 119/199
----------
LR 0.0002
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0547 Acc: 0.7892
0m 11s
Epoch 120/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0502 Acc: 0.7894
0m 11s
Epoch 121/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0542 Acc: 0.7899
0m 11s
Epoch 122/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0500 Acc: 0.7900
0m 11s
Epoch 123/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0509 Acc: 0.7900
0m 11s
Epoch 124/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0556 Acc: 0.7896
0m 11s
Epoch 125/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0535 Acc: 0.7896
0m 11s
Epoch 126/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0506 Acc: 0.7901
0m 11s
Epoch 127/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0508 Acc: 0.7892
0m 11s
Epoch 128/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0474 Acc: 0.7894
0m 11s
Epoch 129/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0495 Acc: 0.7911
0m 11s
Epoch 130/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0543 Acc: 0.7894
0m 11s
Epoch 131/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0562 Acc: 0.7911
0m 11s
Epoch 132/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0499 Acc: 0.7902
0m 11s
Epoch 133/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0530 Acc: 0.7901
0m 11s
Epoch 134/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0489 Acc: 0.7910
0m 11s
Epoch 135/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0534 Acc: 0.7914
0m 11s
Epoch 136/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0544 Acc: 0.7904
0m 11s
Epoch 137/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0575 Acc: 0.7905
0m 11s
Epoch 138/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0611 Acc: 0.7913
0m 11s
Epoch 139/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0583 Acc: 0.7902
0m 11s
Epoch 140/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0532 Acc: 0.7903
0m 11s
Epoch 141/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0535 Acc: 0.7910
0m 11s
Epoch 142/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0607 Acc: 0.7912
0m 11s
Epoch 143/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0636 Acc: 0.7920
saving best model
best_acc 0.792
Model saved to results/cifar10/2021-12-01-19-19-49__resnet34_6839
0m 12s
Epoch 144/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0610 Acc: 0.7898
0m 11s
Epoch 145/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0533 Acc: 0.7900
0m 11s
Epoch 146/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0529 Acc: 0.7910
0m 11s
Epoch 147/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0545 Acc: 0.7913
0m 11s
Epoch 148/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0515 Acc: 0.7896
0m 11s
Epoch 149/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0545 Acc: 0.7910
0m 11s
Epoch 150/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0613 Acc: 0.7894
0m 11s
Epoch 151/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0549 Acc: 0.7917
0m 11s
Epoch 152/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0513 Acc: 0.7920
0m 11s
Epoch 153/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0598 Acc: 0.7909
0m 11s
Epoch 154/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0591 Acc: 0.7902
0m 11s
Epoch 155/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0571 Acc: 0.7906
0m 11s
Epoch 156/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0587 Acc: 0.7901
0m 11s
Epoch 157/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0531 Acc: 0.7917
0m 11s
Epoch 158/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0555 Acc: 0.7917
0m 11s
Epoch 159/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0579 Acc: 0.7895
0m 11s
Epoch 160/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0494 Acc: 0.7901
0m 11s
Epoch 161/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0503 Acc: 0.7902
0m 11s
Epoch 162/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0570 Acc: 0.7893
0m 11s
Epoch 163/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0530 Acc: 0.7899
0m 11s
Epoch 164/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0575 Acc: 0.7891
0m 11s
Epoch 165/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0545 Acc: 0.7904
0m 11s
Epoch 166/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0594 Acc: 0.7904
0m 11s
Epoch 167/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0537 Acc: 0.7924
saving best model
best_acc 0.7924
Model saved to results/cifar10/2021-12-01-19-19-49__resnet34_6839
0m 12s
Epoch 168/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0575 Acc: 0.7906
0m 11s
Epoch 169/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0533 Acc: 0.7916
0m 11s
Epoch 170/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0580 Acc: 0.7911
0m 11s
Epoch 171/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0542 Acc: 0.7902
0m 11s
Epoch 172/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0600 Acc: 0.7898
0m 11s
Epoch 173/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0502 Acc: 0.7899
0m 11s
Epoch 174/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0523 Acc: 0.7896
0m 11s
Epoch 175/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0612 Acc: 0.7920
0m 11s
Epoch 176/199
----------
LR 4e-05
train Loss: 0.0010 Acc: 1.0000
val Loss: 1.0580 Acc: 0.7906
0m 11s
Epoch 177/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0640 Acc: 0.7896
0m 11s
Epoch 178/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0525 Acc: 0.7898
0m 11s
Epoch 179/199
----------
LR 4e-05
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0566 Acc: 0.7910
0m 11s
Epoch 180/199
----------
LR 8.000000000000001e-06
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0595 Acc: 0.7903
0m 11s
Epoch 181/199
----------
LR 8.000000000000001e-06
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0557 Acc: 0.7910
0m 11s
Epoch 182/199
----------
LR 8.000000000000001e-06
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0575 Acc: 0.7903
0m 11s
Epoch 183/199
----------
LR 8.000000000000001e-06
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0563 Acc: 0.7894
0m 11s
Epoch 184/199
----------
LR 8.000000000000001e-06
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0550 Acc: 0.7877
0m 11s
Epoch 185/199
----------
LR 8.000000000000001e-06
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0530 Acc: 0.7896
0m 11s
Epoch 186/199
----------
LR 8.000000000000001e-06
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0538 Acc: 0.7914
0m 11s
Epoch 187/199
----------
LR 8.000000000000001e-06
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0548 Acc: 0.7916
0m 11s
Epoch 188/199
----------
LR 8.000000000000001e-06
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0567 Acc: 0.7915
0m 11s
Epoch 189/199
----------
LR 8.000000000000001e-06
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0568 Acc: 0.7908
0m 11s
Epoch 190/199
----------
LR 8.000000000000001e-06
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0558 Acc: 0.7907
0m 11s
Epoch 191/199
----------
LR 8.000000000000001e-06
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0564 Acc: 0.7901
0m 11s
Epoch 192/199
----------
LR 8.000000000000001e-06
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0550 Acc: 0.7914
0m 11s
Epoch 193/199
----------
LR 8.000000000000001e-06
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0556 Acc: 0.7918
0m 11s
Epoch 194/199
----------
LR 8.000000000000001e-06
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0533 Acc: 0.7904
0m 11s
Epoch 195/199
----------
LR 8.000000000000001e-06
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0504 Acc: 0.7909
0m 11s
Epoch 196/199
----------
LR 8.000000000000001e-06
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0591 Acc: 0.7907
0m 11s
Epoch 197/199
----------
LR 8.000000000000001e-06
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0541 Acc: 0.7916
0m 11s
Epoch 198/199
----------
LR 8.000000000000001e-06
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0497 Acc: 0.7911
0m 11s
Epoch 199/199
----------
LR 8.000000000000001e-06
train Loss: 0.0009 Acc: 1.0000
val Loss: 1.0602 Acc: 0.7903
0m 11s
Best val acc: 0.792400
